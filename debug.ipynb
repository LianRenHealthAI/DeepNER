{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, BertTokenizer\n",
    "  \n",
    "tokenizer = AutoTokenizer.from_pretrained(\"uer/chinese_roberta_L-2_H-128\")\n",
    "\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"uer/chinese_roberta_L-2_H-128\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "{'input_ids': [[101, 2769, 102], [101, 4263, 102], [101, 1266, 102], [101, 776, 102], [101, 1921, 102], [101, 2128, 102], [101, 138, 100, 140, 102]], 'token_type_ids': [[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1, 1, 1]]}"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(['我', '爱', '北', '京', '天', '安', '[BLANK]'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "{'input_ids': tensor([[ 101, 2769, 4263, 1266,  776, 1921, 2128,  138,  100,  140,  102,    0,\n            0,    0,    0,    0,    0,    0,    0,    0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode_plus(['我', '爱', '北', '京', '天', '安', '[BLANK]'],is_split_into_words=True, padding=\"max_length\",\n",
    "                max_length=20,\n",
    "                return_tensors=\"pt\",\n",
    "                truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "{'input_ids': [101, 2769, 4263, 1266, 776, 1921, 2128, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode_plus('我爱北京天安', padding=\"max_length\",\n",
    "                max_length=20,\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer('pretrained/bert-base-chinese/vocab.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('pretrained/bert-base-chinese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_grade_tokenize(raw_text, tokenizer):\n",
    "    \"\"\"\n",
    "    序列标注任务 BERT 分词器可能会导致标注偏移，\n",
    "    用 char-level 来 tokenize\n",
    "    \"\"\"\n",
    "    tokens = []\n",
    "\n",
    "    for _ch in raw_text:\n",
    "        if _ch in [\" \", \"\\t\", \"\\n\"]:\n",
    "            # todo 确定token作用\n",
    "            tokens.append(\"[BLANK]\")  # to do\n",
    "            # tokens.append(_ch)\n",
    "            # tokens.append(\"的\")\n",
    "        else:\n",
    "            if not len(tokenizer.tokenize(_ch)):\n",
    "                tokens.append(\"[INV]\")\n",
    "            else:\n",
    "                tokens.append(_ch)\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'pT2NxMx。'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "['pt', '##2', '##n', '##x', '##m', '##x', '。']"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = BertTokenizer('pretrained/bert-base-chinese/vocab.txt')\n",
    "tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "{'input_ids': [101, 10791, 8144, 8171, 8206, 8175, 8206, 511, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}"
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode_plus(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "{'input_ids': [101, 158, 162, 123, 156, 166, 155, 166, 511, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode_plus(list(text), is_split_into_words=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "['[UNK]', '。']"
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('pretrained/bert-base-chinese')\n",
    "tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "{'input_ids': [101, 100, 511, 102], 'token_type_ids': [0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1]}"
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode_plus(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "{'input_ids': [101, 158, 100, 123, 100, 166, 100, 166, 511, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode_plus(list(text), is_split_into_words=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_grade_tokenize(raw_text, tokenizer):\n",
    "    \"\"\"\n",
    "    序列标注任务 BERT 分词器可能会导致标注偏移，\n",
    "    用 char-level 来 tokenize\n",
    "    \"\"\"\n",
    "    tokens = []\n",
    "\n",
    "    for _ch in raw_text:\n",
    "        if _ch in [\" \", \"\\t\", \"\\n\"]:\n",
    "            # todo 确定token作用\n",
    "            tokens.append(\"[BLANK]\")  # to do\n",
    "            # tokens.append(_ch)\n",
    "            # tokens.append(\"的\")\n",
    "        else:\n",
    "            if not len(tokenizer.tokenize(_ch)):\n",
    "                tokens.append(\"[INV]\")\n",
    "            else:\n",
    "                tokens.append(_ch)\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "['p', 'T', '2', 'N', 'x', 'M', 'x', '。']"
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fine_grade_tokenize(text,tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "{'input_ids': tensor([[101, 100, 511, 102]]), 'token_type_ids': tensor([[0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1]])}"
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(text,return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "name": "python385jvsc74a57bd084c969b26340d7e1ab8a247ed3bb59b9f28e3eaaf50c5ee1f3a95c9d7a2f3a3a"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}